{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b19f9185-b36c-4feb-8a97-199502a4985a",
   "metadata": {},
   "source": [
    "## Nielsen Chapter 1 Notebook\n",
    "### Luke Van Der Male\n",
    "### https://neuralnetworksanddeeplearning.com/chap1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0592f9e5-f422-4734-92d7-69de6900405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "network.py\n",
    "~~~~~~~~~~\n",
    "\n",
    "A module to implement the stochastic gradient descent learning\n",
    "algorithm for a feedforward neural network.  Gradients are calculated\n",
    "using backpropagation.  Note that I have focused on making the code\n",
    "simple, easily readable, and easily modifiable.  It is not optimized,\n",
    "and omits many desirable features.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print (\"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efaba9a7-f4c2-45de-8c74-573a88d25941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b5b8e06-6518-42c6-af1a-3050144bf210",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([784, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfc25847-91b1-4e50-9929-cde81428b99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9043 / 10000\n",
      "Epoch 1: 9237 / 10000\n",
      "Epoch 2: 9284 / 10000\n",
      "Epoch 3: 9326 / 10000\n",
      "Epoch 4: 9379 / 10000\n",
      "Epoch 5: 9372 / 10000\n",
      "Epoch 6: 9402 / 10000\n",
      "Epoch 7: 9403 / 10000\n",
      "Epoch 8: 9437 / 10000\n",
      "Epoch 9: 9376 / 10000\n",
      "Epoch 10: 9442 / 10000\n",
      "Epoch 11: 9465 / 10000\n",
      "Epoch 12: 9440 / 10000\n",
      "Epoch 13: 9463 / 10000\n",
      "Epoch 14: 9455 / 10000\n",
      "Epoch 15: 9479 / 10000\n",
      "Epoch 16: 9488 / 10000\n",
      "Epoch 17: 9499 / 10000\n",
      "Epoch 18: 9500 / 10000\n",
      "Epoch 19: 9493 / 10000\n",
      "Epoch 20: 9491 / 10000\n",
      "Epoch 21: 9493 / 10000\n",
      "Epoch 22: 9497 / 10000\n",
      "Epoch 23: 9501 / 10000\n",
      "Epoch 24: 9519 / 10000\n",
      "Epoch 25: 9497 / 10000\n",
      "Epoch 26: 9495 / 10000\n",
      "Epoch 27: 9512 / 10000\n",
      "Epoch 28: 9493 / 10000\n",
      "Epoch 29: 9512 / 10000\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ffef42-308e-4f43-bb9d-b28432538735",
   "metadata": {},
   "source": [
    "The biases and weights are stored as numpy matrices. So, the following is the weights for the connections between the second and third layer of neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f208f867-c3a1-442f-8064-3ee595951d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02168052,  3.30403423, -1.20574608, -3.76877688, -2.49152873,\n",
       "         4.84049761, -5.66634071,  1.13844771, -5.03587812,  0.07432964,\n",
       "        -2.97537959, -2.80865321,  2.58028027,  6.67081421, -0.356092  ,\n",
       "        -4.86234495, -1.81287792, -1.94196631, -0.81875337, -2.57625658,\n",
       "         1.8620166 ,  2.11554511,  0.99838063, -3.52791335, -2.78598009,\n",
       "        -0.68408559,  1.07599706,  1.31877181, -1.98640913, -3.31610072],\n",
       "       [ 3.93211628,  0.94642555,  0.15622702,  2.66160413, -1.67861087,\n",
       "         3.05783072,  1.86405769, -2.73360653,  0.63231235, -3.80876958,\n",
       "         2.18491922, -8.70054012,  2.92141593, -2.19182498,  2.52880301,\n",
       "        -2.36826952, -2.41453987, -1.84987684,  1.94761907, -2.11309301,\n",
       "         3.00989319, -1.96066865,  3.87240739, -1.54415352,  2.35540851,\n",
       "        -4.41116703,  0.60353547,  1.72379925, -2.04467578,  1.50793735],\n",
       "       [-6.23303584, -0.99287369,  1.58255898,  1.32316225, -6.66312427,\n",
       "        -2.32851009,  6.15614868,  3.36831392, -2.61251553,  3.33570031,\n",
       "        -4.31126366,  3.00522646,  4.62634037, -1.67837665,  0.41647651,\n",
       "        -3.58341567,  0.5486055 ,  0.48531629, -0.7112279 , -6.63235168,\n",
       "         2.56274512,  0.60163429, -3.826136  ,  0.09477779,  2.77161983,\n",
       "         4.89911597, -1.33881382,  1.86195495, -2.13306062,  2.81384043],\n",
       "       [-3.05887198,  0.92621141, -3.85050376, -0.30288479, -1.92981904,\n",
       "        -2.82806654,  4.3897515 , -1.4651225 ,  3.34789293,  0.46953854,\n",
       "         0.28973095,  6.51434642,  3.89669121, -2.2765631 , -4.92551117,\n",
       "        -5.08538333, -1.9061505 ,  4.19062925, -3.77989475, -2.98958305,\n",
       "        -0.19045989, -6.63294673, -3.09421231,  0.59811591, -2.45714339,\n",
       "        -6.35026841, -3.6980817 , -0.89321505, -1.90047769, -6.0614786 ],\n",
       "       [-2.92644556, -5.62606182,  1.88612677, -0.84787557, -4.99977434,\n",
       "         0.31327896, -6.08589454,  7.05436733,  0.35012335, -3.01033738,\n",
       "         1.16094433, -1.89851706,  3.06976743,  0.48302087,  2.4442629 ,\n",
       "         4.19841728, -5.25526868, -1.98408932, -5.22020277, -4.24742921,\n",
       "         5.54115959, -1.72584258,  5.92843064, -2.9046375 , -3.18184342,\n",
       "        -0.31702562,  2.03845951, -2.46764461, -0.80488925,  3.33732226],\n",
       "       [ 9.04339468, -0.50324865, -3.09074071, -5.45593429, -2.56845827,\n",
       "        -7.82942265, -1.58550224,  0.91880541,  0.14504868,  0.93958216,\n",
       "        -0.93479562,  0.97824246, -4.18770799, -6.09291803,  3.40412896,\n",
       "        -5.35342083,  4.55126651, -5.77375546, -0.32779737, -0.66588299,\n",
       "        -0.14410701,  5.09216072,  1.6229238 ,  6.10730841, -1.8671148 ,\n",
       "        -0.52217145,  4.29860629, -0.17991353,  3.20828678,  3.13473469],\n",
       "       [-0.84389928, -2.6007021 ,  3.89353489,  2.20263797, -2.49123213,\n",
       "        -3.81640945, -7.41680866, -1.82872463,  2.09271414, -6.05956075,\n",
       "        -5.81824845,  1.74635814, -5.11020202,  3.33222323, -2.35168315,\n",
       "        -1.60454981,  2.59786047, -0.31005877,  2.73546629, -3.45832947,\n",
       "         2.83955489,  0.76090639,  3.19562758, -3.30651798,  1.6241185 ,\n",
       "         1.05490494, -3.06845161, -1.4059075 , -1.99030314, -3.96736828],\n",
       "       [-1.29286241,  5.78210263,  2.8453236 ,  0.3736683 , -4.25367588,\n",
       "         0.16707202,  6.02457964,  0.98249271, -1.26631765, -0.73077758,\n",
       "         8.13245102,  0.94114987, -2.33986358, -2.29843295, -0.84503638,\n",
       "         5.37406978, -0.57605343, -2.05974765,  5.95009163, -3.47066245,\n",
       "        -6.46442983, -3.35338526, -1.78823117, -2.61816502, -0.03431894,\n",
       "         2.72003283, -0.54775056, -2.4395563 , -2.85459085, -2.17002199],\n",
       "       [-4.44672234, -6.69069579,  7.73292843, -0.05008657, -8.04679343,\n",
       "        -2.36787269, -7.57678615, -3.9031188 , -1.17287096,  1.97485725,\n",
       "         3.93551094,  3.99112158,  0.42885537, -0.47187102,  4.50330427,\n",
       "        -6.47678179, -0.55738864,  3.46860455, -2.05869453, -5.55439882,\n",
       "         2.52753062, -1.0263201 , -2.42868273, -3.76529109, -1.34670378,\n",
       "         0.87373558, -1.4932281 , -2.48835239,  5.5415415 , -3.3276061 ],\n",
       "       [ 2.02975121, -6.59284638, -2.15809121, -0.89995197, -2.82301721,\n",
       "         3.59537527, -9.28150778, -5.56150078,  1.32197103,  0.0107167 ,\n",
       "        -0.45317124,  0.13312655, -0.54786814,  0.49164357,  2.80568473,\n",
       "         7.14850116, -6.53305049,  2.10036583, -8.00634276, -3.36753449,\n",
       "        -4.00484068,  4.32979332, -6.4235827 ,  1.57294989,  1.99078109,\n",
       "        -1.16223408, -0.96903615,  3.29704404, -0.71407745, -2.45881093]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3cf66e-a0ed-4314-b94a-59a763033595",
   "metadata": {},
   "source": [
    "The ordering is a little funny. W(j,k) is the kth neuron in the second layer and the jth neuron in the third layer. This is for vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899747fd-3b34-4e01-af12-0958de69b542",
   "metadata": {},
   "source": [
    "The sigmoid function is just the python version of Equation (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5481ce-99e5-4b6e-9766-d49958093d4d",
   "metadata": {},
   "source": [
    "$\\sigma(z) = \\frac{1}{1 + e^{-z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb5190f0-96a2-447b-a6ef-cfc698fcbfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85032708-25d9-4dbc-b804-948ad599f63b",
   "metadata": {},
   "source": [
    "The feedforward function applies a version of Equation (4) for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a96c3264-e8d5-4578-8241-35d5bba9c389",
   "metadata": {},
   "outputs": [],
   "source": [
    " def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
